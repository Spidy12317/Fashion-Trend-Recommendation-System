{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Queries (Need to Improve Prompt for Keyword Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd          \n",
    "import PIL.Image\n",
    "import google.generativeai as genai     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('models/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"../data/pinterest/a4b7e7fa-fff0-3122-8508-ba87c5c4f042.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY_WORD_EXTRACTION_PROMPT = \"\"\"\n",
    "# System: You are a fashion-savvy individual who enjoys finding trendy clothing online. Your task is to examine an image of a fashion item or outfit and generate search queries that an average person might use when trying to find similar items on shopping websites or search engines.\n",
    "# When generating the search queries, focus on:\n",
    "\n",
    "# The main type of clothing item (e.g., dress, top, pants)\n",
    "# Distinctive colors or patterns\n",
    "# Notable design features that catch the eye\n",
    "# Overall style or occasion the outfit seems suited for\n",
    "\n",
    "# The search queries should be natural and similar to what someone might actually type into a search bar. Avoid overly technical fashion terminology.\n",
    "\n",
    "# Your output should be a list of 5 search queries with the following format:\n",
    "# #Query 1#Query 2#Query 3#Query 4#Query 5\n",
    "\n",
    "# Ensure that each query is realistic and directly relates to the main fashion items shown in the image.\n",
    "# The Each Query should have onlt\n",
    "\n",
    "# NOTE:\n",
    "# The Each Query should be only 2 words long\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# KEY_WORD_EXTRACTION_PROMPT = \"\"\"\n",
    "\n",
    "\n",
    "# System: You are a fashion-savvy individual who enjoys finding trendy clothing online. Your task is to examine an image of a fashion item or outfit and generate short, concise search terms that an average person might use when trying to find similar items on shopping websites or search engines.\n",
    "\n",
    "# When generating the search terms, focus on:\n",
    "\n",
    "# - The main type of clothing item\n",
    "# - Distinctive colors or patterns\n",
    "# - Notable design features\n",
    "# - Overall style or occasion\n",
    "\n",
    "# The search terms should be natural and similar to what someone might actually type into a search bar. Avoid overly technical fashion terminology.\n",
    "\n",
    "# Your output should be a list of 1 search terms with the following format:\n",
    "\n",
    "# #Term1#Term2#Term3#Term4#Term5\n",
    "\n",
    "# Guidelines:\n",
    "# - Each term must be 1 words maximum\n",
    "# - Ensure that each term is realistic and directly relates to the main fashion items shown in the image\n",
    "# - Use terms an average person would type when searching online\n",
    "# - Focus on the most noticeable aspects of the clothing\n",
    "\n",
    "# Example output:\n",
    "# #Red dress#Floral print\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# KEY_WORD_EXTRACTION_PROMPT = \"\"\" \n",
    "# You are an expert image analyst with a deep understanding of fashion and clothing. \n",
    "# You're tasked with generating keywords for an image of an outfit. Please follow these guidelines:\n",
    "\n",
    "# 1. Analyze the image carefully, focusing on the outfit's key features.\n",
    "# 2. Generate up to 5 highly relevant keywords or short phrases that best describe the dress.\n",
    "# 3. Consider the following aspects:\n",
    "#    - Color and pattern\n",
    "#    - Style (e.g., formal, casual, vintage)\n",
    "#    - Fabric type or texture\n",
    "\n",
    "\n",
    "# 4. Ensure each keyword is concise and specific.\n",
    "# 5. Do not include any analysis or explanation, just list the keywords.\n",
    "\n",
    "# Based on the image provided, generate up to 5 keywords or short phrases that best describe the dress:\n",
    "\n",
    "# # #keyword1#keyword2#keyword3#keyword4#keyword5\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "KEY_WORD_EXTRACTION_PROMPT = \"\"\" \n",
    "You are an expert image analyst with a deep understanding of fashion and clothing. \n",
    "You're tasked with generating keywords for an image of an outfit. Please follow these guidelines:\n",
    "\n",
    "1. Analyze the image carefully, focusing on the outfit's key features.\n",
    "2. Generate up to 5 highly relevant keywords or short phrases that best describe the outfit.\n",
    "3. Use general, simple terms that most people would commonly use when searching for similar outfits online.\n",
    "4. Consider the following aspects:\n",
    "   - Color and pattern\n",
    "   - Style (e.g., formal, casual, vintage)\n",
    "   - Fabric type or texture\n",
    "\n",
    "5. Ensure each keyword is concise, specific, and easily understandable to the average person.\n",
    "6. Avoid technical fashion terms or overly specific descriptors.\n",
    "7. Do not include any analysis or explanation, just list the keywords.\n",
    "\n",
    "Based on the image provided, generate up to 5 general, simple keywords or short phrases that best describe the outfit:\n",
    "\n",
    "# #keyword1#keyword2#keyword3#keyword4#keyword5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = model.generate_content([KEY_WORD_EXTRACTION_PROMPT, img], stream=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "from pytrends.exceptions import ResponseError\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keywords =list(map(lambda x: x + \"\", (filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\"))))))\n",
    "search_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payload and capture API tokens. Only needed for interest_over_time(), interest_by_region() & related_queries()\n",
    "attempts, fetched = 0, False\n",
    "while not fetched:\n",
    "    try:\n",
    "        pytrend.build_payload(kw_list=search_keywords,timeframe='today 3-m',geo='IN')\n",
    "        interest_over_time_df = pytrend.interest_over_time()\n",
    "\n",
    "    except ResponseError as err:\n",
    "        print(err)\n",
    "        print(f'Trying again in {15 + 5 * attempts} seconds.')\n",
    "        sleep(60 + 5 * attempts)\n",
    "        attempts += 1\n",
    "        if attempts > 3:\n",
    "            print('Failed after 3 attemps, abort fetching.')\n",
    "            break\n",
    "    else:\n",
    "        fetched = True\n",
    "\n",
    "\n",
    "# Interest Over Time\n",
    "interest_over_time_df.drop(columns=['isPartial'], inplace=True)\n",
    "interest_over_time_df.index = pd.to_datetime(interest_over_time_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing The Trend\n",
    "\n",
    "filtered_df = interest_over_time_df.copy()\n",
    "df_reset = filtered_df.reset_index()\n",
    "df_reset['date_num'] = (df_reset['date'] - df_reset['date'].min()).dt.days\n",
    "df_reset.drop(columns=['date'],inplace=True)\n",
    "\n",
    "main_column = 'date_num'\n",
    "\n",
    "melted_df = df_reset.melt(id_vars=[main_column], var_name='variable', value_name='value')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=melted_df, x=main_column, y='value', hue='variable')\n",
    "plt.title(f'All columns vs {main_column}')\n",
    "plt.xlabel(main_column)\n",
    "plt.ylabel('Values')\n",
    "plt.legend(title='Columns')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_mean(df, window, iterations):\n",
    "    rolling_df = df.copy()\n",
    "    for _ in range(iterations):\n",
    "        rolling_df = rolling_df.rolling(window=window,center=True).mean()\n",
    "    return rolling_df\n",
    "\n",
    "def exponential(x, a, b, c):\n",
    "    return a * np.exp(b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(filtered_df.columns)):\n",
    "    if(filtered_df.columns[index]==\"date_num\"):continue\n",
    "    # Smoothing The Trend Data by using Rolling Mean and Filtering to Include Only 2 months of data\n",
    "\n",
    "\n",
    "    smoothen_df = calculate_rolling_mean(interest_over_time_df, window=20, iterations=2)\n",
    "    smoothen_df = calculate_rolling_mean(smoothen_df, window=10, iterations=1)\n",
    "\n",
    "    smoothen_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    filtered_df = smoothen_df.loc[smoothen_df.index >= pd.to_datetime('today') - pd.DateOffset(months=3)]\n",
    "    filtered_df = filtered_df.reset_index()\n",
    "    filtered_df['date_num'] = (filtered_df['date'] - filtered_df['date'].min()).dt.days\n",
    "    filtered_df.drop(columns=['date'],inplace=True)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(f'Trend Analysis {filtered_df.columns[index]}', fontsize=16)\n",
    "\n",
    "    sns.lineplot(data=filtered_df, x='date_num', y=filtered_df.columns[index],ax=axs[0])\n",
    "    # axs[0].set_title('Interest Over Time')\n",
    "    axs[0].set_xlabel('Date')\n",
    "    axs[0].set_ylabel('Interest')\n",
    "    axs[0].grid(True) \n",
    "\n",
    "\n",
    "    x_data = np.array(filtered_df['date_num'])\n",
    "    y_data = np.array(filtered_df[filtered_df.columns[index]])\n",
    "\n",
    "\n",
    "    first_non_zero_index = np.argmax(y_data > 0) \n",
    "    if(first_non_zero_index):\n",
    "        x_data = x_data[first_non_zero_index-1:]\n",
    "        y_data = y_data[first_non_zero_index-1:]\n",
    "\n",
    "        \n",
    "    if(not np.all(y_data == 0)):\n",
    "        x = x_data\n",
    "        y  = (y_data - y_data.min()) / (y_data.max() - y_data.min())\n",
    "\n",
    "\n",
    "        initial_guess = [1.0, 0.1, 0.0]  # Initial values for a, b, c\n",
    "\n",
    "        # Fit the data to the exponential function with initial guess\n",
    "        popt, pcov = curve_fit(exponential, x, y, p0=initial_guess, maxfev=10000)\n",
    "\n",
    "        # Generate points for the fitted curve\n",
    "        x_fit = np.linspace(min(x), max(x), 100)\n",
    "        y_fit = exponential(x_fit, *popt)\n",
    "\n",
    "        # Calculate R-squared value\n",
    "        r_squared = r2_score(y, exponential(x, *popt))\n",
    "\n",
    "        # Plot the results\n",
    "        axs[1].plot(x, y, label='Data')\n",
    "        axs[1].plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n",
    "        axs[1].legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"R-squared value: {r_squared}\")\n",
    "        print(f\"Fitted parameters: a = {popt[0]}, b = {popt[1]}, c = {popt[2]}\",end=\"\\n--------------------\\n\")\n",
    "\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "        print(\"No Trend Found\",end=\"\\n--------------------\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "         \n",
    "import PIL.Image\n",
    "import google.generativeai as genai     \n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "from pytrends.exceptions import ResponseError\n",
    "from time import sleep\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm_model = genai.GenerativeModel('models/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_keywords_generation_module(image_path:str,model:genai.GenerativeModel)-> list[str]:\n",
    "    \n",
    "    image = PIL.Image.open(image_path)\n",
    "\n",
    "        \n",
    "    KEY_WORD_EXTRACTION_PROMPT = \"\"\" \n",
    "    You are an expert image analyst with a deep understanding of fashion and clothing. \n",
    "    You're tasked with generating keywords for an image of an outfit. Please follow these guidelines:\n",
    "\n",
    "    1. Analyze the image carefully, focusing on the outfit's key features.\n",
    "    2. Generate up to 5 highly relevant keywords or short phrases that best describe the outfit.\n",
    "    3. Use general, simple terms that most people would commonly use when searching for similar outfits online.\n",
    "    4. Consider the following aspects:\n",
    "    - Color and pattern\n",
    "    - Style (e.g., formal, casual, vintage)\n",
    "    - Fabric type or texture\n",
    "\n",
    "    5. Ensure each keyword is concise, specific, and easily understandable to the average person.\n",
    "    6. Avoid technical fashion terms or overly specific descriptors.\n",
    "    7. Do not include any analysis or explanation, just list the keywords.\n",
    "\n",
    "    Based on the image provided, generate up to 5 general, simple keywords or short phrases that best describe the outfit:\n",
    "\n",
    "    # #keyword1#keyword2#keyword3#keyword4#keyword5\n",
    "    \"\"\"\n",
    "    sleep(5)\n",
    "    try:\n",
    "        response = model.generate_content([KEY_WORD_EXTRACTION_PROMPT, image], stream=False)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    if(\"text:\" not in str(response.candidates)): return []\n",
    "\n",
    "    return(list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))\n",
    "\n",
    "\n",
    "def trend_data_extraction_module(search_keywords:list[str],pytrend:TrendReq)-> pd.DataFrame:\n",
    "    \n",
    "    attempts, fetched = 0, False\n",
    "    while not fetched:\n",
    "        try:\n",
    "            pytrend.build_payload(kw_list=search_keywords,timeframe='today 3-m',geo='IN')\n",
    "            interest_over_time_df = pytrend.interest_over_time()\n",
    "            interest_over_time_df.drop(columns=['isPartial'], inplace=True)\n",
    "            interest_over_time_df.index = pd.to_datetime(interest_over_time_df.index)\n",
    "            return interest_over_time_df\n",
    "\n",
    "\n",
    "        except:\n",
    "            print(f'Trying again in {15 + 5 * attempts} seconds.')\n",
    "            sleep(10 + 5 * attempts)\n",
    "            attempts += 1\n",
    "            if attempts > 4:\n",
    "                print('Failed after 4 attemps, abort fetching.')\n",
    "                break\n",
    "        else:\n",
    "            fetched = True\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def trend_visualization(df:pd.DataFrame):\n",
    "    filtered_df = df.copy()\n",
    "    df_reset = filtered_df.reset_index()\n",
    "    df_reset['date_num'] = (df_reset['date'] - df_reset['date'].min()).dt.days\n",
    "    df_reset.drop(columns=['date'],inplace=True)\n",
    "\n",
    "    main_column = 'date_num'\n",
    "\n",
    "    melted_df = df_reset.melt(id_vars=[main_column], var_name='variable', value_name='value')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=melted_df, x=main_column, y='value', hue='variable')\n",
    "    plt.title(f'All columns vs {main_column}')\n",
    "    plt.xlabel(main_column)\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend(title='Columns')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def calculate_rolling_mean(df:pd.DataFrame, window:int, iterations:int)-> pd.DataFrame:\n",
    "    rolling_df = df.copy()\n",
    "    for _ in range(iterations):\n",
    "        rolling_df = rolling_df.rolling(window=window,center=True).mean()\n",
    "    return rolling_df\n",
    "\n",
    "\n",
    "def exponential(x:int, a:int, b:int, c:int)->int:\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "\n",
    "def curve_fiting_module(df:pd.DataFrame,show_plots:bool=True) -> dict[str:[int,list]]:\n",
    "    \n",
    "    fitted_trends_df = pd.DataFrame(columns=[\"Keywords\",\"R2 Score\",\"Slope\",\"Trending\"])\n",
    "    \n",
    "    for index in range(len(df.columns)):   \n",
    "        \n",
    "        if(df.columns[index]==\"date_num\"):continue\n",
    "        # Smoothing The Trend Data by using Rolling Mean and Filtering to Include Only 2 months of data\n",
    "\n",
    "        print(\"Serach Keyword: \",df.columns[index])\n",
    "\n",
    "        smoothen_df = calculate_rolling_mean(df, window=20, iterations=2)\n",
    "        smoothen_df = calculate_rolling_mean(smoothen_df, window=10, iterations=1)\n",
    "\n",
    "        smoothen_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "        filtered_df = smoothen_df.loc[smoothen_df.index >= pd.to_datetime('today') - pd.DateOffset(months=3)]\n",
    "        filtered_df = filtered_df.reset_index()\n",
    "        filtered_df['date_num'] = (filtered_df['date'] - filtered_df['date'].min()).dt.days\n",
    "        filtered_df.drop(columns=['date'],inplace=True)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'Trend Analysis {filtered_df.columns[index]}', fontsize=16)\n",
    "\n",
    "        sns.lineplot(data=filtered_df, x='date_num', y=filtered_df.columns[index],ax=axs[0])\n",
    "        # axs[0].set_title('Interest Over Time')\n",
    "        axs[0].set_xlabel('Date')\n",
    "        axs[0].set_ylabel('Interest')\n",
    "        axs[0].grid(True)\n",
    "\n",
    "\n",
    "        x_data = np.array(filtered_df['date_num'])\n",
    "        y_data = np.array(filtered_df[filtered_df.columns[index]])\n",
    "\n",
    "\n",
    "        first_non_zero_index = np.argmax(y_data > 0) \n",
    "        if(first_non_zero_index):\n",
    "            x_data = x_data[first_non_zero_index-1:]\n",
    "            y_data = y_data[first_non_zero_index-1:]\n",
    "\n",
    "            \n",
    "        if(not np.all(y_data == 0)):\n",
    "            x = x_data\n",
    "            y  = (y_data - y_data.min()) / (y_data.max() - y_data.min())\n",
    "\n",
    "\n",
    "            initial_guess = [1.0, 0.1, 0.0]  # Initial values for a, b, c\n",
    "\n",
    "            # Fit the data to the exponential function with initial guess\n",
    "            popt, pcov = curve_fit(exponential, x, y, p0=initial_guess, maxfev=10000)\n",
    "\n",
    "            # Generate points for the fitted curve\n",
    "            x_fit = np.linspace(min(x), max(x), 100)\n",
    "            y_fit = exponential(x_fit, *popt)\n",
    "\n",
    "            # Calculate R-squared value\n",
    "            r_squared = r2_score(y, exponential(x, *popt))\n",
    "\n",
    "            # Plot the results\n",
    "            axs[1].plot(x, y, label='Data')\n",
    "            axs[1].plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n",
    "            axs[1].legend()\n",
    "            \n",
    "\n",
    "            row  = {\"Keywords\":df.columns[index],\"R2 Score\":r_squared,\"Slope\":popt[0],\"Trending\":popt[0]>=0}\n",
    "            fitted_trends_df = pd.concat([fitted_trends_df,pd.DataFrame([row])],ignore_index=True)\n",
    "\n",
    "\n",
    "            print(f\"R-squared value: {r_squared}\")\n",
    "            print(f\"Fitted parameters: a = {popt[0]}, b = {popt[1]}, c = {popt[2]}\")\n",
    "\n",
    "        else:\n",
    "            \n",
    "            row  = {\"Keywords\":df.columns[index],\"R2 Score\":0,\"Slope\":-1,\"Trending\":None}\n",
    "            fitted_trends_df = pd.concat([fitted_trends_df,pd.DataFrame([row])],ignore_index=True)\n",
    "\n",
    "            print(\"No Trend Found\")\n",
    "\n",
    "        \n",
    "        if show_plots: plt.show()\n",
    "        else: plt.close(fig)\n",
    "        print(\"\\n---------------------\\n\")\n",
    "\n",
    "\n",
    "    return fitted_trends_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_analysis_pipeline(df:pd.DataFrame,image_folder_path:str,genai_model:genai.GenerativeModel,pytrend:TrendReq,visualize_trend:bool = False)->pd.DataFrame:\n",
    "    \n",
    "    raw_data_for_prediction_df = pd.DataFrame(columns=[\"Image ID\",\"Image Source\", \"Image Link\",\"Keywords\",\"R2 Score\",\"Slope\",\"Trending\"])\n",
    "    \n",
    "    # raw_data_for_prediction_df = pd.read_csv(\"./output.csv\")\n",
    "    # Image_ID = list(raw_data_for_prediction_df['Image ID'])\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        image_id,image_src,image_link = row \n",
    "        \n",
    "        image_path = f\"{image_folder_path}/{image_id}.png\"\n",
    "\n",
    "        # if(image_path in Image_ID):continue\n",
    "\n",
    "        search_keywords = query_keywords_generation_module(image_path,genai_model)\n",
    "        if(len(search_keywords)==0):continue\n",
    "        \n",
    "        trend_df = trend_data_extraction_module(search_keywords,pytrend)\n",
    "        if(trend_df.empty):continue\n",
    "\n",
    "        if(visualize_trend):trend_visualization(trend_df)\n",
    "        fitted_trend_data = curve_fiting_module(trend_df,show_plots=visualize_trend)\n",
    "\n",
    "\n",
    "        fitted_trend_data[\"Image ID\"] = image_id\n",
    "        fitted_trend_data[\"Image Source\"] = image_src\n",
    "        fitted_trend_data[\"Image Link\"] = image_link\n",
    "        raw_data_for_prediction_df = pd.concat([raw_data_for_prediction_df,fitted_trend_data],ignore_index=False)\n",
    "\n",
    "        raw_data_for_prediction_df.to_csv(\"output.csv\")\n",
    "        \n",
    "    return raw_data_for_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usecase Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = \"../data/myntra\"\n",
    "pinterest_df = pd.read_csv(f\"{image_folder_path}/myntra_trend.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_for_prediction_df = trend_analysis_pipeline(pinterest_df,image_folder_path,llm_model,pytrend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion_trend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
