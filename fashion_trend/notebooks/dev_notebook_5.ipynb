{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path):\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_text_embedding(model, processor, text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "def get_image_embedding(model, processor, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"../CLIP\" \n",
    "embedding_model, embedding_processor = load_clip_model(model_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import uuid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LocalVectorStore:\n",
    "    def __init__(self):\n",
    "        self.indexes = {}\n",
    "        self.vectors = {}\n",
    "        self.metadata = {}\n",
    "\n",
    "    def create_index(self, index_name, dimension):\n",
    "        if index_name in self.indexes:\n",
    "            raise ValueError(f\"Index '{index_name}' already exists\")\n",
    "        \n",
    "        self.indexes[index_name] = faiss.IndexFlatL2(dimension)\n",
    "        self.vectors[index_name] = []\n",
    "        self.metadata[index_name] = []\n",
    "        logger.info(f\"Created index '{index_name}' with dimension {dimension}\")\n",
    "\n",
    "    def add_vector(self, index_name, vector, metadata=None):\n",
    "        if index_name not in self.indexes:\n",
    "            raise ValueError(f\"Index '{index_name}' does not exist\")\n",
    "        \n",
    "        vector = np.array(vector).astype('float32').reshape(1, -1)\n",
    "        self.indexes[index_name].add(vector)\n",
    "        self.vectors[index_name].append(vector)\n",
    "        self.metadata[index_name].append(metadata)\n",
    "        logger.info(f\"Added vector to index '{index_name}'\")\n",
    "\n",
    "    def search(self, index_name, query_vector, k=5):\n",
    "        if index_name not in self.indexes:\n",
    "            raise ValueError(f\"Index '{index_name}' does not exist\")\n",
    "        \n",
    "        logger.info(f\"Searching in index '{index_name}' for {k} nearest neighbors\")\n",
    "        \n",
    "        try:\n",
    "            query_vector = np.array(query_vector).astype('float32').reshape(1, -1)\n",
    "            \n",
    "            if self.indexes[index_name].ntotal == 0:\n",
    "                logger.warning(f\"Index '{index_name}' is empty. No search performed.\")\n",
    "                return []\n",
    "\n",
    "            distances, indices = self.indexes[index_name].search(query_vector, min(k, self.indexes[index_name].ntotal))\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                results.append({\n",
    "                    'vector': self.vectors[index_name][idx].tolist(),  # Convert to list for JSON serialization\n",
    "                    'metadata': self.metadata[index_name][idx],\n",
    "                    'distance': float(distances[0][i])  # Convert to float for JSON serialization\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"Found {len(results)} results\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save(self, filepath):\n",
    "        try:\n",
    "            data = {\n",
    "                'vectors': {k: [v.tolist() for v in vecs] for k, vecs in self.vectors.items()},\n",
    "                'metadata': self.metadata\n",
    "            }\n",
    "            \n",
    "            index_data = {}\n",
    "            for index_name, index in self.indexes.items():\n",
    "                index_data[index_name] = faiss.serialize_index(index).tobytes()\n",
    "            \n",
    "            data['index_data'] = index_data\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            logger.info(f\"Vector store saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving vector store: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            vector_store = cls()\n",
    "            vector_store.vectors = {k: [np.array(v, dtype='float32') for v in vecs] for k, vecs in data['vectors'].items()}\n",
    "            vector_store.metadata = data['metadata']\n",
    "\n",
    "            for index_name, index_bytes in data['index_data'].items():\n",
    "                index = faiss.deserialize_index(np.frombuffer(index_bytes, dtype='uint8'))\n",
    "                vector_store.indexes[index_name] = index\n",
    "\n",
    "            logger.info(f\"Vector store loaded from {filepath}\")\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading vector store: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# vector_store = LocalVectorStore()\n",
    "\n",
    "# # Create indexes for different sets\n",
    "# vector_store.create_index(\"set1\", dimension=128)\n",
    "# vector_store.create_index(\"set2\", dimension=64)\n",
    "\n",
    "# # Add vectors to set1\n",
    "# vector_store.add_vector(\"set1\", np.random.rand(128), metadata={\"id\": 1, \"name\": \"Vector 1\"})\n",
    "# vector_store.add_vector(\"set1\", np.random.rand(128), metadata={\"id\": 2, \"name\": \"Vector 2\"})\n",
    "\n",
    "# # Add vectors to set2\n",
    "# vector_store.add_vector(\"set2\", np.random.rand(64), metadata={\"id\": 1, \"name\": \"Vector A\"})\n",
    "# vector_store.add_vector(\"set2\", np.random.rand(64), metadata={\"id\": 2, \"name\": \"Vector B\"})\n",
    "\n",
    "# # Save the vector store\n",
    "# vector_store.save(\"vector_store.pkl\")\n",
    "\n",
    "# # Load the vector store\n",
    "# loaded_vector_store = LocalVectorStore.load(\"vector_store.pkl\")\n",
    "\n",
    "# # Search for similar vectors using the loaded store\n",
    "# query_vector = np.random.rand(128)\n",
    "# results = loaded_vector_store.search(\"set1\", query_vector, k=2)\n",
    "\n",
    "# print(\"Search results:\")\n",
    "# for result in results:\n",
    "#     print(f\"Vector: {result['vector']}\")\n",
    "#     print(f\"Metadata: {result['metadata']}\")\n",
    "#     print(f\"Distance: {result['distance']}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NumPyVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = {}\n",
    "        self.metadata = {}\n",
    "\n",
    "    def create_index(self, index_name, dimension):\n",
    "        if index_name in self.vectors:\n",
    "            raise ValueError(f\"Index '{index_name}' already exists\")\n",
    "        \n",
    "        self.vectors[index_name] = []\n",
    "        self.metadata[index_name] = []\n",
    "        logger.info(f\"Created index '{index_name}' with dimension {dimension}\")\n",
    "\n",
    "    def add_vector(self, index_name, vector, metadata=None):\n",
    "        if index_name not in self.vectors:\n",
    "            raise ValueError(f\"Index '{index_name}' does not exist\")\n",
    "        \n",
    "        vector = np.array(vector).astype('float32')\n",
    "        self.vectors[index_name].append(vector)\n",
    "        self.metadata[index_name].append(metadata)\n",
    "        logger.info(f\"Added vector to index '{index_name}'\")\n",
    "\n",
    "    def search(self, index_name, query_vector, k=5):\n",
    "        if index_name not in self.vectors:\n",
    "            raise ValueError(f\"Index '{index_name}' does not exist\")\n",
    "        \n",
    "        logger.info(f\"Searching in index '{index_name}' for {k} nearest neighbors\")\n",
    "        \n",
    "        query_vector = np.array(query_vector).astype('float32')\n",
    "        vectors = np.array(self.vectors[index_name])\n",
    "        \n",
    "        if len(vectors) == 0:\n",
    "            logger.warning(f\"Index '{index_name}' is empty. No search performed.\")\n",
    "            return []\n",
    "\n",
    "        distances = np.linalg.norm(vectors - query_vector, axis=1)\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in nearest_indices:\n",
    "            results.append({\n",
    "                'vector': self.vectors[index_name][idx].tolist(),\n",
    "                'metadata': self.metadata[index_name][idx],\n",
    "                'distance': float(distances[idx])\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Found {len(results)} results\")\n",
    "        return results\n",
    "\n",
    "    def save(self, filepath):\n",
    "        try:\n",
    "            data = {\n",
    "                'vectors': {k: [v.tolist() for v in vecs] for k, vecs in self.vectors.items()},\n",
    "                'metadata': self.metadata\n",
    "            }\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            logger.info(f\"Vector store saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving vector store: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            vector_store = cls()\n",
    "            vector_store.vectors = {k: [np.array(v, dtype='float32') for v in vecs] for k, vecs in data['vectors'].items()}\n",
    "            vector_store.metadata = data['metadata']\n",
    "\n",
    "            logger.info(f\"Vector store loaded from {filepath}\")\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading vector store: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# vector_store = NumPyVectorStore()\n",
    "\n",
    "# # Create index\n",
    "# vector_store.create_index(\"purchase_data\", dimension=128)\n",
    "\n",
    "# # Add vectors\n",
    "# vector_store.add_vector(\"purchase_data\", np.random.rand(128), metadata={\"id\": 1, \"product\": \"Book\"})\n",
    "# vector_store.add_vector(\"purchase_data\", np.random.rand(128), metadata={\"id\": 2, \"product\": \"Laptop\"})\n",
    "\n",
    "# # Save the vector store\n",
    "# vector_store.save(\"vector_store.pkl\")\n",
    "\n",
    "# # Load the vector store\n",
    "# loaded_vector_store = NumPyVectorStore.load(\"vector_store.pkl\")\n",
    "\n",
    "# # Search for similar vectors\n",
    "# query_vector = np.random.rand(128)\n",
    "# results = loaded_vector_store.search(\"purchase_data\", query_vector, k=2)\n",
    "\n",
    "# print(\"Search results:\")\n",
    "# for result in results:\n",
    "#     print(f\"Vector: {result['vector'][:5]}...\")  # Showing only first 5 elements\n",
    "#     print(f\"Metadata: {result['metadata']}\")\n",
    "#     print(f\"Distance: {result['distance']}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = NumPyVectorStore()\n",
    "namespace = uuid.NAMESPACE_DNS\n",
    "vector_db_folder = \"../data/app_data/vector_indices\"\n",
    "\n",
    "folder_paths = [\"../data/app_data/purchase_data\",\"../data/app_data/available_stocks\",\"../data/app_data/wardrobe_images\"]\n",
    "for folder_path in folder_paths:\n",
    "    index = folder_path.split(\"/\")[-1]\n",
    "    vector_store.create_index(index, dimension=768)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if(file.endswith(\"png\")):\n",
    "            image_path = f\"{folder_path}/{file}\"\n",
    "\n",
    "            image_id = uuid.uuid3(namespace, image_path)\n",
    "\n",
    "            image_embeddings = get_image_embedding(embedding_model,embedding_processor,image_path)\n",
    "            vector_store.add_vector(index, image_embeddings[0], metadata={\"id\":image_id , \"image_path\": image_path})\n",
    "\n",
    "vector_store.save(f\"{vector_db_folder}/vector_db.pkl\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = get_image_embedding(embedding_model,embedding_processor,\"/Users/t.sumukhflexday/Desktop/Projects/Test/fashion_trend/fashion_trend/data/app_data/purchase_data/purchase_hist_1.png\")\n",
    "query_vector = np.array(query_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vector_store = NumPyVectorStore.load(\"../data/app_data/vector_indices/vector_db.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loaded_vector_store.search(\"available_stocks\",np.random.rand(768),k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(result['metadata']['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion_trend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
