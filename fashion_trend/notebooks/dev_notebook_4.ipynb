{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path):\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_text_embedding(model, processor, text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "def get_image_embedding(model, processor, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP\" \n",
    "model, processor = load_clip_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/Trend Analysis/pinterest\"\n",
    "\n",
    "image_embeddings = {}\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.png'):\n",
    "            test = os.path.join(root, file)\n",
    "            image_embeddings[file] = get_image_embedding(model,processor,os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"stylish Belt\"\n",
    "text_embedding = get_text_embedding(model,processor,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in image_embeddings:\n",
    "    similarity = calculate_similarity(image_embeddings[img], text_embedding)\n",
    "    print(img,similarity,end=\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path):\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "def get_text_embedding(model, processor, text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "def get_image_embedding(model, processor, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"../CLIP\" \n",
    "embedding_model, embedding_processor = load_clip_model(model_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase History Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Purchase History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../data/Recommendation/purchase_data/image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Complimentary Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd          \n",
    "import PIL.Image\n",
    "import google.generativeai as genai     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm_model = genai.GenerativeModel('models/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUGGEST_ITEM_PROMPT = \"\"\" \n",
    "# Analyze the following image of a fashion product:\n",
    "\n",
    "# Suggest 2 complementary fashion items that would pair well with this product, considering factors such as color coordination, style consistency, occasion appropriateness, and seasonal relevance.\n",
    "\n",
    "# Format the response as follows, using '#' as a delimiter. Do not provide any descriptions or explanations, only the names of the items:\n",
    "\n",
    "# #Complementary Item 1\n",
    "# #Complementary Item 2\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "SUGGEST_ITEM_PROMPT = \"\"\" \n",
    "Analyze the following image of a fashion product:\n",
    "\n",
    "Suggest 2 complementary fashion items that would pair well with this product, considering factors such as color coordination, style consistency, occasion appropriateness, and seasonal relevance.\n",
    "\n",
    "Format the response as follows, using '#' as a delimiter:\n",
    "\n",
    "#Complementary Item 1\n",
    "#Complementary Item 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = llm_model.generate_content([SUGGEST_ITEM_PROMPT, image], stream=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions = (list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/Recommendation/available_stocks\"\n",
    "\n",
    "image_embeddings = {}\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.png'):\n",
    "            print(file)\n",
    "            image_embeddings[file] = get_image_embedding(model,processor,os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embd = get_text_embedding(model,processor,suggestions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Top 5 Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = []\n",
    "for img in image_embeddings:\n",
    "    similarity = calculate_similarity(image_embeddings[img], text_embd)\n",
    "    search.append((similarity,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "import google.generativeai as genai     \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path:str)-> tuple[CLIPModel,CLIPProcessor]:\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def get_text_embedding(model:CLIPModel, processor:CLIPProcessor, text:str)->torch.Tensor:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image_embedding(model:CLIPModel, processor:CLIPProcessor, image_path:str)->torch.Tensor:\n",
    "    image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1:torch.Tensor, embedding2:torch.Tensor)->float:\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "def generate_suggestion_based_on_purchase_history(image:str,model:genai.GenerativeModel)->list[str]:\n",
    "\n",
    "    SUGGEST_ITEM_PROMPT = \"\"\" \n",
    "    Analyze the following image of a fashion product:\n",
    "\n",
    "    Suggest 2 complementary fashion items that would pair well with this product, considering factors such as color coordination, style consistency, occasion appropriateness, and seasonal relevance.\n",
    "\n",
    "    Format the response as follows, using '#' as a delimiter:\n",
    "\n",
    "    #Complementary Item 1\n",
    "    #Complementary Item 2\n",
    "    \"\"\"\n",
    "\n",
    "    sleep(5)\n",
    "    try:\n",
    "        response = model.generate_content([SUGGEST_ITEM_PROMPT, image], stream=False)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    if(\"text:\" not in str(response.candidates)): return []\n",
    "\n",
    "    return(list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))\n",
    "\n",
    "\n",
    "def generate_image_vector_embeddings(folder_path:str,model:CLIPModel,processor:CLIPProcessor)->dict[str,torch.Tensor]:\n",
    "    image_embeddings = {}\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path)):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.png'):\n",
    "                image_embeddings[file] = get_image_embedding(model,processor,os.path.join(root, file))\n",
    "\n",
    "    return image_embeddings\n",
    "\n",
    "\n",
    "def text_image_retriever(text:str,image_embeddings:dict[str,torch.Tensor],model:CLIPModel,processor:CLIPProcessor,top_k:int=5)->list[list[int,str]]:\n",
    "    \n",
    "    text_embd = get_text_embedding(model,processor,text)\n",
    "    search = []\n",
    "    for img in image_embeddings:\n",
    "        similarity = calculate_similarity(image_embeddings[img], text_embd)\n",
    "        search.append((similarity,img))\n",
    "    \n",
    "    search.sort(reverse=True)\n",
    "\n",
    "    return search[:top_k]\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP\" \n",
    "embedding_model, embedding_processor = load_clip_model(model_path)\n",
    "\n",
    "avaliable_stock_path = \"../data/Recommendation/available_stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm_model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "\n",
    "image_embeddings = generate_image_vector_embeddings(avaliable_stock_path,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fashion_items_for_purchase_history(purchased_item_image:Image,image_embeddings:dict[str,torch.Tensor],llm_model:genai.GenerativeModel,embedding_model:CLIPModel,embedding_processor:CLIPProcessor) -> list[str]:\n",
    "\n",
    "    fashion_item_suggestion = generate_suggestion_based_on_purchase_history(purchased_item_image,llm_model)\n",
    "    if(len(fashion_item_suggestion)==0): return {-1:\"Something Went Wrong\"}\n",
    "\n",
    "    recommendations = []\n",
    "    for suggested_item_description in fashion_item_suggestion:\n",
    "        suggestions = text_image_retriever(suggested_item_description,image_embeddings,embedding_model,embedding_processor,top_k=3)\n",
    "        recommendations.extend([i[1] for i in suggestions if i[1] not in recommendations])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_in_jupyter(list_images:list[Image], max_images:int=5, figsize:tuple[int,int]=(7, 5))->None:\n",
    "    # Limit the number of images to display\n",
    "    list_images = list_images[:max_images]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot grid\n",
    "    n_images = len(list_images)\n",
    "    n_cols = min(3, n_images)  # Max 3 columns\n",
    "    n_rows = (n_images - 1) // n_cols + 1\n",
    "    \n",
    "    # Create a new figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    fig.suptitle(\"Recommendations\", fontsize=16)\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for i, img in enumerate(list_images):\n",
    " \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Image\\n{img.format}, {img.size}, {img.mode}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchased_item_image = Image.open(\"../data/Recommendation/purchase_data/image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchased_item_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_fashion_items_for_purchase_history(purchased_item_image,image_embeddings,llm_model,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for image_id in recommendations:\n",
    "    products.append(Image.open(f\"{avaliable_stock_path}/{image_id}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_in_jupyter(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Based Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "import google.generativeai as genai     \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path:str)-> tuple[CLIPModel,CLIPProcessor]:\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def get_text_embedding(model:CLIPModel, processor:CLIPProcessor, text:str)->torch.Tensor:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image_embedding(model:CLIPModel, processor:CLIPProcessor, image_path:str)->torch.Tensor:\n",
    "    image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1:torch.Tensor, embedding2:torch.Tensor)->float:\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "def generate_suggestion_based_on_purchase_history(image:str,model:genai.GenerativeModel)->list[str]:\n",
    "\n",
    "    SUGGEST_ITEM_PROMPT = \"\"\" \n",
    "    Analyze the following image of a fashion product:\n",
    "\n",
    "    Suggest 2 complementary fashion items that would pair well with this product, considering factors such as color coordination, style consistency, occasion appropriateness, and seasonal relevance.\n",
    "\n",
    "    Format the response as follows, using '#' as a delimiter:\n",
    "\n",
    "    #Complementary Item 1\n",
    "    #Complementary Item 2\n",
    "    \"\"\"\n",
    "\n",
    "    sleep(5)\n",
    "    try:\n",
    "        response = model.generate_content([SUGGEST_ITEM_PROMPT, image], stream=False)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    if(\"text:\" not in str(response.candidates)): return []\n",
    "\n",
    "    return(list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))\n",
    "\n",
    "\n",
    "def generate_image_vector_embeddings(folder_path:str,model:CLIPModel,processor:CLIPProcessor)->dict[str,torch.Tensor]:\n",
    "    image_embeddings = {}\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path)):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.png'):\n",
    "                image_embeddings[file] = get_image_embedding(model,processor,os.path.join(root, file))\n",
    "\n",
    "    return image_embeddings\n",
    "\n",
    "\n",
    "def text_image_retriever(text:str,image_embeddings:dict[str,torch.Tensor],model:CLIPModel,processor:CLIPProcessor,top_k:int=5)->list[list[int,str]]:\n",
    "    \n",
    "    text_embd = get_text_embedding(model,processor,text)\n",
    "    search = []\n",
    "    for img in image_embeddings:\n",
    "        similarity = calculate_similarity(image_embeddings[img], text_embd)\n",
    "        search.append((similarity,img))\n",
    "    \n",
    "    search.sort(reverse=True)\n",
    "\n",
    "    return search[:top_k]\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP\" \n",
    "embedding_model, embedding_processor = load_clip_model(model_path)\n",
    "\n",
    "avaliable_stock_path = \"../data/Recommendation/available_stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm_model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "\n",
    "image_embeddings = generate_image_vector_embeddings(avaliable_stock_path,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fashion_items_for_text_description(text_description:str,image_embeddings:dict[str,torch.Tensor],embedding_model:CLIPModel,embedding_processor:CLIPProcessor) -> list[str]:\n",
    "\n",
    "    suggestions = text_image_retriever(text_description,image_embeddings,embedding_model,embedding_processor,top_k=3)\n",
    "    suggestions = [i[1] for i in suggestions]\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_in_jupyter(list_images:list[Image], max_images:int=5, figsize:tuple[int,int]=(7, 5))->None:\n",
    "    # Limit the number of images to display\n",
    "    list_images = list_images[:max_images]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot grid\n",
    "    n_images = len(list_images)\n",
    "    n_cols = min(3, n_images)  # Max 3 columns\n",
    "    n_rows = (n_images - 1) // n_cols + 1\n",
    "    \n",
    "    # Create a new figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    fig.suptitle(\"Recommendations\", fontsize=16)\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for i, img in enumerate(list_images):\n",
    " \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Image\\n{img.format}, {img.size}, {img.mode}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am looking for pink tops\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_fashion_items_for_text_description(text,image_embeddings,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for image_id in recommendations:\n",
    "    products.append(Image.open(f\"{avaliable_stock_path}/{image_id}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_in_jupyter(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo Based Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "import google.generativeai as genai     \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_path:str)-> tuple[CLIPModel,CLIPProcessor]:\n",
    "    model = CLIPModel.from_pretrained(model_path, local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path, local_files_only=True)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def get_text_embedding(model:CLIPModel, processor:CLIPProcessor, text:str)->torch.Tensor:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image_embedding(model:CLIPModel, processor:CLIPProcessor, image_path:str=\"\",image:Image=None)->torch.Tensor:\n",
    "    \n",
    "    if(image == None):\n",
    "        image = Image.open(image_path)\n",
    "    # image = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def calculate_similarity(embedding1:torch.Tensor, embedding2:torch.Tensor)->float:\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "\n",
    "def generate_suggestion_based_on_purchase_history(image:str,model:genai.GenerativeModel)->list[str]:\n",
    "\n",
    "    SUGGEST_ITEM_PROMPT = \"\"\" \n",
    "    Analyze the following image of a fashion product:\n",
    "\n",
    "    Suggest 2 complementary fashion items that would pair well with this product, considering factors such as color coordination, style consistency, occasion appropriateness, and seasonal relevance.\n",
    "\n",
    "    Format the response as follows, using '#' as a delimiter:\n",
    "\n",
    "    #Complementary Item 1\n",
    "    #Complementary Item 2\n",
    "    \"\"\"\n",
    "\n",
    "    sleep(5)\n",
    "    try:\n",
    "        response = model.generate_content([SUGGEST_ITEM_PROMPT, image], stream=False)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    if(\"text:\" not in str(response.candidates)): return []\n",
    "\n",
    "    return(list(filter(lambda x: x != \"\" and x!= '<Search Queries:>',map(str.strip,response.text.split(\"#\")))))\n",
    "\n",
    "\n",
    "def generate_image_vector_embeddings(folder_path:str,model:CLIPModel,processor:CLIPProcessor)->dict[str,torch.Tensor]:\n",
    "    image_embeddings = {}\n",
    "    for root, dirs, files in tqdm(os.walk(folder_path)):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.png'):\n",
    "                image_embeddings[file] = get_image_embedding(model,processor,os.path.join(root, file))\n",
    "\n",
    "    return image_embeddings\n",
    "\n",
    "\n",
    "def image_image_retriever(search_image:Image,image_embeddings:dict[str,torch.Tensor],model:CLIPModel,processor:CLIPProcessor,top_k:int=5)->list[list[int,str]]:\n",
    "    \n",
    "    image_embd = get_image_embedding(model,processor,image=search_image)\n",
    "    search = []\n",
    "    for img in image_embeddings:\n",
    "        similarity = calculate_similarity(image_embeddings[img], image_embd)\n",
    "        search.append((similarity,img))\n",
    "    \n",
    "    search.sort(reverse=True)\n",
    "\n",
    "    return search[:top_k]\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP\" \n",
    "embedding_model, embedding_processor = load_clip_model(model_path)\n",
    "\n",
    "avaliable_stock_path = \"../data/Recommendation/available_stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv('API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "llm_model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "\n",
    "image_embeddings = generate_image_vector_embeddings(avaliable_stock_path,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_fashion_items_for_image(item_image:Image,image_embeddings:dict[str,torch.Tensor],embedding_model:CLIPModel,embedding_processor:CLIPProcessor) -> list[str]:\n",
    "\n",
    "    suggestions = image_image_retriever(item_image,image_embeddings,embedding_model,embedding_processor,top_k=3)\n",
    "    suggestions = [i[1] for i in suggestions ]\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_in_jupyter(list_images:list[Image], max_images:int=5, figsize:tuple[int,int]=(7, 5))->None:\n",
    "    # Limit the number of images to display\n",
    "    list_images = list_images[:max_images]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot grid\n",
    "    n_images = len(list_images)\n",
    "    n_cols = min(3, n_images)  # Max 3 columns\n",
    "    n_rows = (n_images - 1) // n_cols + 1\n",
    "    \n",
    "    # Create a new figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    fig.suptitle(\"Recommendations\", fontsize=16)\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten() if n_images > 1 else [axes]\n",
    "    \n",
    "    for i, img in enumerate(list_images):\n",
    " \n",
    "        # Display the image\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Image\\n{img.format}, {img.size}, {img.mode}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_item_image = Image.open(\"../data/Recommendation/purchase_data/image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_fashion_items_for_image(search_item_image,image_embeddings,embedding_model,embedding_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for image_id in recommendations:\n",
    "    products.append(Image.open(f\"{avaliable_stock_path}/{image_id}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_in_jupyter(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion_trend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
